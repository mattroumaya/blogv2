[
  {
    "objectID": "posts/rummy/index.html",
    "href": "posts/rummy/index.html",
    "title": "Infinite Rummy in the Age of Quarantine",
    "section": "",
    "text": "In these unprecedented times, my wife Molly and I have been playing infinite rounds of Rummy ♠️ ♥️ ♣️ ♦️. Since I don’t have any high-quality posts yet (and probably won’t any time soon💩), I’m just going to graph our endless rounds until we’re out of the weeds. My friend and former colleague (and data pro!) Steph showed me how she was graphing nightly games of Rummy 500 with her wife, and I found the idea too fun to not steal 😺!\nI spent all weekend getting up and running with the deadly blogdown + Hugo combo, and I’m ready to share my trivial data insights with the world!\nMolly and I love board games like 7 Wonders Duel, Codenames, and some good ol’ Catan when we can round up… anyone else…but we have quickly come to terms that we’ll be riding out the stay at home order for quite a while.\n(If you’re only looking for cats and kittens, scroll to the end)\nLet’s take a first glimpse at our scorecard data:\n\n\n\nMessy Data With Competitive Annotation\n\n\nWhile the raw data explicitly shows that I’m a loser, we’ll see if that’s actually the case after some cleaning and viz.\nHere’s a tribble of our cumulative rounds:\n\nlibrary(tidyverse)\n\nrummy <- tribble(\n  ~Round,~Molly,~Matt,\n  1,159,167,\n  2,90,100,\n  3,55,139,\n  4,91,-25,\n  5,98,44,\n  6,97,64,\n  7,68,68,\n  8,102,90,\n  9,53,85,\n  10,-21,134,\n  11,88,144,\n  12,-23,106,\n  13,63,111,\n  14,91,128,\n  15,108,79,\n  16,88,76,\n  17,100,146,\n  18,124,-10,\n  19,66,41,\n  20,77,105,\n  21,38,95,\n  22,91,13,\n  23,92,117,\n  24,65,101,\n  25,88,142\n)\n\nTo build some suspense before we get to the cumulative sum and see who the real loser 😭 is, we actually need to utilize one of my new favorite functions pivot_longer(), which will help us visualize our rounds.\n\nrummy_wider <- rummy %>%\n  pivot_longer(cols = c(\"Molly\",\"Matt\"), values_to = \"Score\") %>%\n  rename(Player = name)\n\nNow our data looks like this:\n\n\n\n\n \n  \n    Round \n    Player \n    Score \n  \n \n\n  \n    1 \n    Molly \n    159 \n  \n  \n    1 \n    Matt \n    167 \n  \n  \n    2 \n    Molly \n    90 \n  \n  \n    2 \n    Matt \n    100 \n  \n  \n    3 \n    Molly \n    55 \n  \n  \n    3 \n    Matt \n    139 \n  \n\n\n\n\n\nNice! Let’s have a first look at how we stack up round-to-round:\n\nlibrary(wesanderson)\n\nrummy_wider %>%\n  group_by(Player) %>%\n  ggplot(aes(x=Round,y=Score,group=Player,color=Player)) +\n  scale_x_continuous(breaks = seq(1,25,1)) +\n  geom_line(size=1.25) +\n  scale_color_manual(values=wes_palette(name = \"Moonrise3\")) +\n  theme_minimal() +\n  labs(title = \"...And in That Moment, I swear That We Were (Playing) Infinite (Rummy)\",\n          subtitle = paste0(\"Score by Round for \",max(rummy_wider$Round),\" Rounds\")) \n\n\n\n\nLast night during a bout of insomnia, I came across the new CRAN release for ggbump and had to give it a shot, and am pretty excited to find some more uses for it. This definitely isn’t the best visualization I’ve made, and would have been more interesting with a third player (maybe the thruple from HGTV’s House Hunters?)\nAnyway, to work with ggbump, we just need to rank each round. We did tie during round 7, so ties.method = random will make the decision for us.\n\nlibrary(ggbump)\nlibrary(cowplot)\nlibrary(wesanderson)\n\nrummy_bump <- rummy_wider %>%\n  group_by(Round) %>%\n  mutate(rank = rank(Score,ties.method = \"random\")) %>%\n  ungroup()\n\nggplot(rummy_bump, aes(Round,rank,color = Player)) +\n  geom_point(size = 7) +\n  geom_bump(size = 2, smooth = 4)+\n  scale_x_continuous(limits = c(-0.6,25.6),breaks = seq(1,25,1)) +\n  theme_minimal_grid(font_size = 14, line_size = 0)+\n  theme(panel.grid.major = element_blank(),\n        axis.ticks = element_blank()) +\n  scale_y_continuous(breaks = c(1,2), labels = c(\"1\" = \"Loser\", \"2\" = \"Winner\")) +\n  labs(y = \"\") +\n  scale_color_manual(values = wes_palette(n=2, name = \"Moonrise3\"))\n\n\n\n\nI know I said it’s not the best visualization I’ve made, but I still think it looks pretty cool, and is easy enough to see the change from win to loss.\nAnd now, the moment you’ve all been waiting for!\nHere’s the current scoreboard for wins per round:\n\nscoreboard <- rummy_bump %>%\n  mutate(Wins = rank - 1,\n         Wins = ifelse(Wins == 0,\"Loss\",\"Win\")) %>%\n  arrange(desc(Wins)) %>%\n  select(Player,Wins)\n\nkable(table(scoreboard))\n\n\n\n \n  \n      \n    Loss \n    Win \n  \n \n\n  \n    Matt \n    10 \n    15 \n  \n  \n    Molly \n    15 \n    10 \n  \n\n\n\n\n\nWe still haven’t decided how we’re going to end the game, and with no end to the pandemic in sight, we still have a lot more Rummy in our future. Should we play Rummy 5,000? 25,000? BILLIONS?🙄\nAnd one more graph of our cumulative scores, for good measure:\n\ncumulative <- rummy_wider %>%\n  group_by(Player) %>%\n  mutate(Cumulative = cumsum(Score))\n\ncumulative %>%\n  group_by(Player) %>%\n  ggplot(aes(x=Round,y=Cumulative,group=Player,color=Player)) +\n  scale_x_continuous(breaks = seq(1,25,1)) +\n  geom_line(size=1.25) +\n  geom_text(data = cumulative %>%\n              group_by(Player) %>%\n              arrange(desc(Cumulative)) %>%\n              slice(1),\n            aes(label = Cumulative), show.legend = FALSE,vjust=-1) +\n  scale_color_manual(values=wes_palette(name = \"Moonrise3\")) +\n  theme_minimal() +\n  labs(title = \"...And in That Moment, I swear That We Were (Playing) Infinite (Rummy)\",\n          subtitle = paste0(\"Cumulative Score for \",max(cumulative$Round),\" Rounds\"),\n       y = \"Cumulative Score\")\n\n\n\n\nAnd as a final note, this post and website were conceptualized on 4/26, which is Phil and Mango’s 5th birthday!! 👽\n🎂 🎈 🎁 HAPPY BIRTHDAY TO MANGO AND PHIL!🎂 🎈 🎁\n\n\n\n\n\n\nBaby Brothers\n\n\n\n\n\n\n\nBigger Baby Brothers"
  },
  {
    "objectID": "posts/rename-columns/index.html",
    "href": "posts/rename-columns/index.html",
    "title": "52 Different Ways to Rename a Column in R",
    "section": "",
    "text": "Not that my previous posts were intellectual thinkpieces, but I thought that I had to write about something novel or innovative to provide any level of value.\nWhen I first starting using R, my code was a mash-up of base R, dplyr, and data.table. I would reference a column by index and then by name. It was hard for me to follow, and I cringe at the idea that I sent some of this old code to colleagues.\nI was trying to think of how many ways there are to do simple data cleaning tasks in R, and thought it would be fun to explore.\nThe only task accomplished in the rest of this post will be renaming a column, and some pics of my cats.\n\nOriginal column name: old_column\nRenamed column name: new_column\n\nEvery example will include a data.frame that is called df and will contain one column named old_column that we will rename as new_column:\n\n\n\n\n\n\n  \n  \n    \n      old_column\n    \n  \n  \n    Here\n    is\n    a\n    column\n  \n  \n  \n\n\n\n\n\nUsing Base R\nThe following examples will only use base R, meaning no additional packages will be required to run this code.\n\nCall colnames on df and index the first column.\n\n\ncolnames(df)[1] <- \"new_column\"\n\n\nCall names on df and index the first column.\n\n\nnames(df)[1] <- \"new_column\"\n\n\nCall colnames on df and subset the first column also using colnames.\n\n\ncolnames(df)[colnames(df) == \"old_column\"] <- \"new_column\"\n\n\nCall names on df and subset the first column also using names.\n\n\nnames(df)[names(df) == \"old_column\"] <- \"new_column\"\n\n\nCall colnames on df and subset the first column using names.\n\n\ncolnames(df)[names(df) == \"old_column\"] <- \"new_column\"\n\n\nCall names on df and subset the first column using colnames.\n\n\nnames(df)[colnames(df) == \"old_column\"] <- 'new_column'\n\n\nCall colnames on df and subset using logical indexing which. This returns the index of the column that is equal to “old_column”.\n\n\ncolnames(df)[which(colnames(df) == \"old_column\")] <- \"new_column\"\n\n\nSince df only has one column, we can also call names on df:\n\n\nnames(df) <- \"new_column\"\n\n\n…or colnames on df:\n\n\ncolnames(df) <- \"new_column\"\n\n\nWe can also use a different, and less efficient approach. Instead of renaming the column value, we can create a new column that is identical to old_column and name it new_column. Then we can remove old_column from our df:\n\n\n# Create a new column called \"new_column\" that is an exact copy of \"old_column\"\ndf$new_column <- df$old_column\n\n# Remove \"old_column\"\ndf$old_column <- NULL\n\n\nGetting a bit more abstract, we can use colnames with grepl to use regex pattern matching:\n\n\ncolnames(df)[grepl(\"old\", colnames(df))] <- \"new_column\"\n\n\n…we can also use names with #11:\n\n\nnames(df)[grepl(\"old\", names(df))] <- \"new_column\"\n\n\nWe can swap the first names with colnames:\n\n\ncolnames(df)[grepl(\"old\", names(df))] <- \"new_column\"\n\n\nFlip it and reverse it…\n\n\n\nnames(df)[grepl(\"old\", colnames(df))] <- \"new_column\"\n\n\nUsing grep + names:\n\n\nnames(df)[grep(\"old\", names(df))] <- \"new_column\"\n\n\nUsing grep + colnames:\n\n\ncolnames(df)[grep(\"old\", colnames(df))] <- \"new_column\"\n\n\nUsing grep + names then colnames:\n\n\nnames(df)[grep(\"old\", colnames(df))] <- \"new_column\"\n\n\nUsing grep + colnames then names:\n\n\n(I am intentionally stopping myself from more Missy Elliott references.)\n\n\ncolnames(df)[grep(\"old\", names(df))] <- \"new_column\"\n\n\nUsing sub + colnames:\n\n\ncolnames(df) <- sub(\"old_column\", \"new_column\", colnames(df))\n\n\nUsing sub + names:\n\n\nnames(df) <- sub(\"old_column\", \"new_column\", names(df))\n\n\nUsing sub + names then colnames:\n\n\nnames(df) <- sub(\"old_column\", \"new_column\", colnames(df))\n\n\nUsing sub + colnames then names:\n\n\ncolnames(df) <- sub(\"old_column\", \"new_column\", names(df))\n\n\nUsing gsub + colnames:\n\n\ncolnames(df) <- gsub(\"old_column\", \"new_column\", colnames(df))\n\n\nUsing gsub + names:\n\n\nnames(df) <- gsub(\"old_column\", \"new_column\", names(df))\n\n\nUsing gsub + names then colnames:\n\n\nnames(df) <- gsub(\"old_column\", \"new_column\", colnames(df))\n\n\nUsing gsub + colnames then names:\n\n\ncolnames(df) <- gsub(\"old_column\", \"new_column\", names(df))\n\n\nUsing a for loop with colnames:\n\n\nfor (i in paste0(\"new_column\")){\n  colnames(df) <- i\n}\n\n\nUsing a for loop with names:\n\n\nfor (i in paste0(\"new_column\")){\n  names(df) <- i\n}\n\n\nUsing setNames:\n\n\ndf <- setNames(df, \"new_column\")\n\n\nUsing eval and parse with names:\n\n\neval(parse(text = 'names(df) <- \"new_column\"'))\n\n\nUsing eval and parse with colnames:\n\n\neval(parse(text = 'colnames(df) <- \"new_column\"'))\n\n\nUsing setNames and replace:\n\n\nsetNames(df, replace(names(df), names(df) == 'old_column', 'new_column'))\n\n\nUsing transform:\n\n\ndf <- transform(df, new_column = old_column, old_column = NULL)\n\n\n\ntidyverse\nYou can learn more about the tidyverse here\n\nUsing rename without a %>%:\n\n\ndf <- rename(df, \"new_column\" = \"old_column\")\n\n\nUsing rename with a %>%:\n\n\ndf <- df %>% \n  rename(\"new_column\" = \"old_column\")\n\n\nRenaming in a select call without a %>%:\n\n\ndf <- select(df, \"new_column\" = \"old_column\")\n\n\nRenaming in a select call with a %>%:\n\n\ndf <- df %>% \n  select(\"new_column\" = \"old_column\")\n\n\nUsing mutate to create a new column and then removing the old_column:\n\n\ndf <- df %>% \n  mutate(new_column = old_column) %>% \n  select(-old_column)\n\n\nUsing mutate to create a new column and then removing the old_column without pipes (%>%):\n\n\ndf <- mutate(df, new_column = old_column)\ndf$old_column <- NULL\n\n\nUsing purrr + setnames and str_replace_*:\n\n\ndf <- df %>%\n    set_names(~(.) %>%\n                  str_replace_all(\"old_column\", \"new_column\"))\n\n\nUsing a character vector and rename:\n\n\nrename_vec <- c(\"new_column\" = \"old_column\")\n\ndf <- df %>% \n  rename(rename_vec)\n\n\nUsing str_replace + names:\n\n\nnames(df) <- str_replace(names(df), \"old_column\", \"new_column\")\n\n\nUsing str_replace + colnames:\n\n\ncolnames(df) <- str_replace(colnames(df), \"old_column\", \"new_column\")\n\n\nUsing starts_with:\n\n\ndf <- df %>% \n  select(\"new_column\" = starts_with(\"old\"))\n\n\nUsing ends_with:\n\n\ndf <- df %>% \n  select(\"new_column\" = ends_with(\"column\"))\n\n\nUsing rename_with + gsub:\n\n\ndf <- df %>% \n  rename_with(~gsub(\"old_\", \"new_\", .x))\n\n\nUsing rename_with + sub:\n\n\ndf <- df %>% \n  rename_with(~sub(\"old_\", \"new_\", .x))\n\n\nUsing rename_with and str_replace:\n\n\ndf <- df %>% \n     rename_with(~str_replace(\"new_column\", \"old_column\", .x))\n\n\nRename with an index:\n\n\ndf <- df %>% \n     rename(\"new_column\" = 1)\n\nA note: I’m going to stop interchanging names and colnames as I did previously. I didn’t have any idea how many ways there would be to rename columns when I started this, but it’s becoming evident that there are likely hundreds of ways if we count every nuance.\nI’m also throwing in the towel on the deprecated/superseded rename_at / rename_if / rename_all functions, since they have been replaced by select and rename_with.\n\n\ndata.table\ndata.table is really fast, and you can… do cool stuff with it. I am a data.table n00b. You can learn more about data.table here.\n\nUsing data.table::setnames:\n\n\ndf <- as.data.table(df, keep.rownames = FALSE)\nsetnames(df, \"old_column\", \"new_column\")\n\n\nUsing data.table::setnames with an index:\n\n\ndf <- as.data.table(df, keep.rownames = FALSE)\nsetnames(df, 1, \"new_column\")\n\n\nRefactoring the previous data.table example (I have no idea what I’m doing 😅)\n\n\nas.data.table(df)[, .(new_column = old_column)]\n\n\n\nWhat’s in a (re)name?\nR is an amazing language and there are endless things you can do. Coming from SPSS, I was previously familiar with rename and just left it at that. I had some grand ideas of microbenchmarking each of these methods to find the fastest renaming solution, and maybe that will happen someday if I get an espresso machine or something. ☕\nOur team at work will be transitioning from SPSS to R, and this has given me a lot to think about, specifically about the importance of having standardized code, but also having some built-in flexibility for each person’s coding style. I’m looking forward to another version of this post, where I focus on a task that is slightly more complicated. Maybe iterating through a data.frame column/rowwise?\nI also acknowledge my severe lack of data.table knowledge. I don’t work with big data, and am not in a position to need to make production-level code performant. tidyverse code is way more intuitive for me, and the community is really supportive and engaged, so I will likely leave data.table off the …table for a while.\n… I’ll see myself out.\n\n\nCats\n\n\n\nMango\n\n\n\n\n\nPhil\n\n\n\n\nReferences\n\nhttps://stackoverflow.com/questions/7531868/how-to-rename-a-single-column-in-a-data-frame\nhttps://stackoverflow.com/questions/35084427/how-to-change-column-names-in-dataframe-in-the-loop\nhttps://stackoverflow.com/questions/50687741/how-to-rename-column-headers-in-r\nhttps://stackoverflow.com/questions/46616591/rename-multiple-dataframe-columns-using-purrr\nhttps://stackoverflow.com/questions/20987295/rename-multiple-columns-by-names\nhttps://stackoverflow.com/questions/9283171/rename-multiple-dataframe-columns-referenced-by-current-names/9292258\nhttps://stackoverflow.com/questions/53168572/how-to-rename-specific-variable-of-a-data-frame-with-setnames"
  },
  {
    "objectID": "posts/dealing-with-doubleheadr/index.html",
    "href": "posts/dealing-with-doubleheadr/index.html",
    "title": "Tidying The Survey Monkey Doubleheader",
    "section": "",
    "text": "SurveyMonkey 🐒 is a popular online survey development software that outputs a very frustrating kind of response file. If you’ve ever received exported results from SurveyMonkey, you probably know what I’m talking about.\nWhen response data is exported as a .csv or .xlsx file, it looks something like this:\n\n\n\nThe Doubleheader\n\n\nYou can see that the response data for the question “Please provide your contact information:” contains several inputs for Name, Company, Address, etc. Deleting the first row might seem to make sense at first, but when your survey contains dozens of questions with multiple question types, things can get out of hand really fast.\nThis post is an overview of one approach for cleaning up SurveyMonkey columns. I struggled with this for a while, but now have a workflow that seems pretty efficient, especially if your goal is to set up RMarkdown reports while your survey is being administered. If it is helpful for you, that’s awesome! If you have a better way of approaching this issue, please let me know!\n\nThe workflow\n\nA function that creates column names that are concatenated from the question + response in_snake_case (or camelCase, or whatever) using the aptly-named janitor package.\n\nCleaning up any annoying/extremely_long_column_names_from_very_long_questions.\n\nIf needed, subsetting columns into dataframes or lists for analyses/visualization. (The dplyr functions starts_with and ends_with work really well with this workflow).\n\nThat’s it, now you’re ready to do some meaningful analyses!\n\nLet’s start by viewing our response data. You’ll need to use the packages tidyverse, janitor, and readxl if you’re using .xlsx.\n(I always save the original version without any changes so I can build in QC checks, and tend to use _preserve as an identifier.) You’ll see that some columns are missing values, and will be read in with “…#”\n**Edit in 10/2022: I’ve added sample data to avoid loading .csv/.xlsx files. You can grab it by running:\n\n# devtools::install_github('mattroumaya/doubleheadr')\nlibrary(doubleheadr)\nsurvey_data <- doubleheadr::demo\nsurvey_data_preserve <- doubleheadr::demo\n\n\nsurvey_data %>% \n  head(4) %>% \n  gt::gt()\n\n\n\n\n\n  \n  \n    \n      Respondent ID\n      Please provide your contact information:\n      ...3\n      ...4\n      ...5\n      ...6\n      ...7\n      ...8\n      ...9\n      ...10\n      ...11\n      I wish it would have snowed more this winter.\n    \n  \n  \n    NA\nName\nCompany\nAddress\nAddress 2\nCity/Town\nState/Province\nZIP/Postal Code\nCountry\nEmail Address\nPhone Number\nResponse\n    11385284375\nBenjamin Franklin\nPoor Richard's\nNA\nNA\nPhiladelphia\nPA\n19104\nNA\nbenjamins@gmail.com\n215-555-4444\nStrongly disagree\n    11385273621\nMae Jemison\nNASA\nNA\nNA\nDecatur\nAlabama\n20104\nNA\nmjemison@nasa.gov\n221-134-4646\nStrongly agree\n    11385258069\nCarl Sagan\nSmithsonian\nNA\nNA\nWashington\nD.C.\n33321\nNA\nstargazer@gmail.com\n999-999-4422\nNeither agree nor disagree\n  \n  \n  \n\n\n\n\n\n\nUgh! Look at those terrible column names. 😠\n\n\n\nOof!!!\n\n\n\ndouble_header <- function(x) {\n  \n  df <- as_tibble(x)\n  \n  keydat <- df %>%\n    slice(1) %>%\n    select_if(negate(is.na)) %>%\n    pivot_longer(everything()) %>%\n    group_by(grp = cumsum(!startsWith(name, \"...\"))) %>%\n    mutate(value = sprintf(\"%s (%s)\", first(name), value)) %>%\n    ungroup %>%\n    select(-grp)\n  \n  df <- df %>%\n    rename_at(vars(keydat$name), ~ keydat$value) %>%\n    slice(-1) %>%\n    clean_names()\n}\n\n\n\nThe double header breakdown\n\nslice(1) selects the first row, which contains the other names we need.\n\nselect_if(negate(is.na)) selects all columns where the first row is not NA, because we don’t need to alter these column names.\n\npivot_longer(everything()) transforms our dataframe from wide to long, and automatically creates the columns name and value.\n\n\n\n(name holds all of our column names and value holds all of our secondary column names.)\n\n\n\n# A tibble: 6 × 2\n  name                                     value         \n  <chr>                                    <chr>         \n1 Please provide your contact information: Name          \n2 ...3                                     Company       \n3 ...4                                     Address       \n4 ...5                                     Address 2     \n5 ...6                                     City/Town     \n6 ...7                                     State/Province\n\n\n\ngroup_by(grp = cumsum(!startsWith(name, \"...\"))) groups rows and then applies a cumulative sum for all rows in the name column that do not start with “…” until a row other than “…” is encountered. This is better shown in the table below:\n\n\n\n\n\n\n\n  \n  \n    \n      name\n      value\n    \n  \n  \n    \n      1\n    \n    Please provide your contact information:\nName\n    ...3\nCompany\n    ...4\nAddress\n    ...5\nAddress 2\n    ...6\nCity/Town\n    ...7\nState/Province\n    ...8\nZIP/Postal Code\n    ...9\nCountry\n    ...10\nEmail Address\n    ...11\nPhone Number\n    \n      2\n    \n    I wish it would have snowed more this winter.\nResponse\n  \n  \n  \n\n\n\n\n\nmutate(value = sprintf(\"%s (%s)\", first(name), value)) updates our value column and concatenates our names so that they’re meaningful. We’re almost there!\n\n\n\n\n\n\n\n  \n  \n    \n      name\n      value\n    \n  \n  \n    \n      1\n    \n    Please provide your contact information:\nPlease provide your contact information: (Name)\n    ...3\nPlease provide your contact information: (Company)\n    ...4\nPlease provide your contact information: (Address)\n    ...5\nPlease provide your contact information: (Address 2)\n    ...6\nPlease provide your contact information: (City/Town)\n    ...7\nPlease provide your contact information: (State/Province)\n  \n  \n  \n\n\n\n\n\nThen we just ungroup and drop the grp column.\n\nFinally, we rename the columns in our survey_data by using our updated names in keydat$value, and call clean_names() to convert to snake_case 🐍\n\n\n\nAll together now\nLet’s run the function on survey_data\n\nsurvey_data <- double_header(survey_data)\n\nHere’s a comparison of our original names and our cleaned names:\n\n\n\n\n\n\n  \n  \n    \n      Old Names\n      New Names\n    \n  \n  \n    Respondent ID\nrespondent_id\n    Please provide your contact information:\nplease_provide_your_contact_information_name\n    ...3\nplease_provide_your_contact_information_company\n    ...4\nplease_provide_your_contact_information_address\n    ...5\nplease_provide_your_contact_information_address_2\n    ...6\nplease_provide_your_contact_information_city_town\n    ...7\nplease_provide_your_contact_information_state_province\n    ...8\nplease_provide_your_contact_information_zip_postal_code\n    ...9\nplease_provide_your_contact_information_country\n    ...10\nplease_provide_your_contact_information_email_address\n    ...11\nplease_provide_your_contact_information_phone_number\n    I wish it would have snowed more this winter.\ni_wish_it_would_have_snowed_more_this_winter_response\n  \n  \n  \n\n\n\n\n\n\nSubsetting made easy!\nNow if we want to subset data based on certain questions/columns, we can do it really easily using starts_with and ends_with.\nSometimes it’s easier to rename columns so that they’re shorter and easier to work with, and sometimes it’s fine to keep some really long column names if your survey contains a lot of similar questions.\nBelow, we’ll combine all of the questions that start with please_provide_your_contact_information and shorten the names to only start with contact_information + value.\nstarts_with example:\n\ncontact_info <- survey_data %>%\n  select(starts_with(\"please_provide_your_contact_information\")) %>%\n  rename_at(vars(starts_with(\"please\")), ~str_remove(.,\"please_provide_your_\"))\n\ncontact_info %>% \n  head() %>% \n  gt::gt()\n\n\n\n\n\n  \n  \n    \n      contact_information_name\n      contact_information_company\n      contact_information_address\n      contact_information_address_2\n      contact_information_city_town\n      contact_information_state_province\n      contact_information_zip_postal_code\n      contact_information_country\n      contact_information_email_address\n      contact_information_phone_number\n    \n  \n  \n    Benjamin Franklin\nPoor Richard's\nNA\nNA\nPhiladelphia\nPA\n19104\nNA\nbenjamins@gmail.com\n215-555-4444\n    Mae Jemison\nNASA\nNA\nNA\nDecatur\nAlabama\n20104\nNA\nmjemison@nasa.gov\n221-134-4646\n    Carl Sagan\nSmithsonian\nNA\nNA\nWashington\nD.C.\n33321\nNA\nstargazer@gmail.com\n999-999-4422\n    W. E. B. Du Bois\nNAACP\nNA\nNA\nGreat Barrington\nMA\n1230\nNA\ndubois@web.com\n999-000-1234\n    Florence Nightingale\nPublic Health Co\nNA\nNA\nFlorence\nIT\n33225\nNA\nfirstnurse@aol.com\n123-456-7899\n    Galileo Galilei\nNASA\nNA\nNA\nPisa\nIT\n12345\nNA\ngalileo123@yahoo.com\n111-888-9944\n  \n  \n  \n\n\n\n\nends_with is really helpful in combination with our double_header function and SurveyMonkey data, because open-ended or free-text responses all end with …open_ended_response\nDepending on the report or project you’re building, it’s sometimes useful to add all of your free-text responses as dataframes in a list, to include as an appendix, or to select important comments to include in an executive summary.\nends_with example:\n\nopen_ended <- survey_data %>%\n  select(ends_with(\"response\")) \n\nprint(names(open_ended))\n\n[1] \"i_wish_it_would_have_snowed_more_this_winter_response\"\n\n\nThat’s it for this post! I’d love to hear from you if you found this workflow helpful, or if there is any way it could be improved.\nSome ideas for future posts include building this function into a package (for the sole purpose of learning how to build R packages), showing a few tricks I’ve learned with the HH and lattice packages for visualizing Likert scale responses, and some more trivial posts about Rummy, House Hunters, and any other reality TV my wife and I are currently fixated on."
  },
  {
    "objectID": "posts/parameterized-reports/index.html",
    "href": "posts/parameterized-reports/index.html",
    "title": "Using {foreach} to Speed up Parameterized RMarkdown PDF Report",
    "section": "",
    "text": "Recently, I have been working on a project to find alternate methods for creating PDF score reports for assessments that have typically been made using Microsoft Access. As someone who has literally never had a fun time working in Access, I was thrilled to be assigned to this project, and was then quickly humbled by the task at hand.\nIt has been easy enough to create parameterized HTML reports for a handful of internal users, but when the audience is external at a scale of thousands of reports, the processing time in generating the reports quickly becomes a major consideration in operationalizing the solution.\nAfter tweaking code, unsuccessfully experimenting with cache = TRUE, and several cups of coffee ☕, I finally found a reasonable solution using the foreach and doParallel packages as suggested by a colleague, in reference to an email from 2018 from a different organization dealing with the same exact dilemma.\nThe first thing I learned that really helped understand why my PDF reports were being compiled so slowly is that R is single threaded by default. This is a new concept to me so I’m not going to even pretend to fully understand it, but I found this resource to be super helpful.\nFor this quick demo, we will need to make an RMarkdown file and an R file."
  },
  {
    "objectID": "posts/parameterized-reports/index.html#rmarkdown-file-report-layout.rmd",
    "href": "posts/parameterized-reports/index.html#rmarkdown-file-report-layout.rmd",
    "title": "Using {foreach} to Speed up Parameterized RMarkdown PDF Report",
    "section": "RMarkdown File: Report-Layout.rmd",
    "text": "RMarkdown File: Report-Layout.rmd\nThe RMarkdown file could look something like this:\n---\ntitle: \"Iris Demo\"\noutput: pdf_document\nparams: \n  species: \"\"\n---\n\n```{r}\nknitr::opts_chunk$set(\n    echo = FALSE,\n    message = FALSE,\n    warning = FALSE\n)\n\nlibrary(tidyverse)\n\ndf <- iris %>% \n  filter(Species == params$species)\n```\n\nReport for the species: params$species\n\n```{r}\ndf %>% \n  filter(Species == \"setosa\") %>% \n  ggplot(aes(Sepal.Width))+\n  geom_histogram() +\n  ggtitle(paste0(\"Distribution of Sepal.Length for \", params$species))\n```\nThe parameters for species will be passed through to generate a report for each species in iris.\nWhen creating thousands of reports, this process takes quite a while, and luckily we can speed it up using foreach and doParallel."
  },
  {
    "objectID": "posts/parameterized-reports/index.html#r-script-create-reports",
    "href": "posts/parameterized-reports/index.html#r-script-create-reports",
    "title": "Using {foreach} to Speed up Parameterized RMarkdown PDF Report",
    "section": "R Script: Create Reports",
    "text": "R Script: Create Reports\nThe R script could look something like this:\n\nlibrary(tidyverse)\nlibrary(foreach)\nlibrary(doParallel)\n\nspecies_names <- as.character(unique(iris$Species))\n\n\nn_cores <- parallel::detectCores()\ncluster <- parallel::makeCluster(n_cores-1)                 \ndoParallel::registerDoParallel(cluster)\n\nforeach (i = seq_along(species_names), .combine = 'c') %dopar% {\n  system.time(rmarkdown::render(\"Report-Layout.rmd\",\n                    params = list(species = species_names[i]),\n                    output_file = paste0(\"Report for \", species_names[i])))\n}\n\nTo generate reports, all we need to do is run the Create Reports.r script, and we’ll have three reports generated in a matter of seconds. As we increase the volume of reports, the multi-core processing enabled by foreach / doParallel will significantly cut down on the processing time.\nI had intended for this to be longer but really just want to put this out there, and maybe revisit some day with a Part 2. For now, Molly and me are going to hike around Wissahickon with Donut 🐶 🍩\n\n\n\nOur dog, Donut"
  },
  {
    "objectID": "posts/doubleheadR/index.html",
    "href": "posts/doubleheadR/index.html",
    "title": "doubleheadR",
    "section": "",
    "text": "The past few months of work-life have been constantly busy with survey design and analysis. COVID has really changed the landscape for just about everything, and leaders of projects and departments want and need to implement change quickly, and survey research is helping to inform speedy decision making.\nI’m here to demo the first R package that I’ve ever written that has been helping me quickly clean and tidy data from SurveyMonkey to prepare for analysis and reporting. This is also a continuation of my previous post about tidying the SurveyMonkey Double Header."
  },
  {
    "objectID": "posts/doubleheadR/index.html#step-1-install-packages",
    "href": "posts/doubleheadR/index.html#step-1-install-packages",
    "title": "doubleheadR",
    "section": "Step 1: install packages",
    "text": "Step 1: install packages\nI’ll assume you are familiar with installing from GitHub, and I’m working on being more concise.\n\nlibrary(devtools)\ninstall_github('mattroumaya/doubleheadr')"
  },
  {
    "objectID": "posts/doubleheadR/index.html#step-2-check-out-the-demo-file",
    "href": "posts/doubleheadR/index.html#step-2-check-out-the-demo-file",
    "title": "doubleheadR",
    "section": "Step 2: check out the demo file",
    "text": "Step 2: check out the demo file\n\nlibrary(tidyverse)\nlibrary(doubleheadr)\n\ndemo <- doubleheadr::demo\n\ndoubleheadr comes with a built-in demo data set, which helps illustrate the two functions available: clean_headr and trim_headr. The data set demo mimics a .xlsx export from SurveyMonkey and looks like this:\n\n\n\n\n \n  \n    Respondent ID \n    Please provide your contact information: \n    ...3 \n    ...4 \n    ...5 \n    ...6 \n    ...7 \n    ...8 \n    ...9 \n    ...10 \n    ...11 \n    I wish it would have snowed more this winter. \n  \n \n\n  \n    NA \n    Name \n    Company \n    Address \n    Address 2 \n    City/Town \n    State/Province \n    ZIP/Postal Code \n    Country \n    Email Address \n    Phone Number \n    Response \n  \n  \n    11385284375 \n    Benjamin Franklin \n    Poor Richard's \n    NA \n    NA \n    Philadelphia \n    PA \n    19104 \n    NA \n    benjamins@gmail.com \n    215-555-4444 \n    Strongly disagree \n  \n  \n    11385273621 \n    Mae Jemison \n    NASA \n    NA \n    NA \n    Decatur \n    Alabama \n    20104 \n    NA \n    mjemison@nasa.gov \n    221-134-4646 \n    Strongly agree \n  \n  \n    11385258069 \n    Carl Sagan \n    Smithsonian \n    NA \n    NA \n    Washington \n    D.C. \n    33321 \n    NA \n    stargazer@gmail.com \n    999-999-4422 \n    Neither agree nor disagree \n  \n\n\n\n\n\nAs analyzers of this data, we will want to paste the column names and values in the first row together. It’s a bit more complicated than that, which is detailed in my previous post, but we’re being concise here."
  },
  {
    "objectID": "posts/doubleheadR/index.html#step-3-clean_headr",
    "href": "posts/doubleheadR/index.html#step-3-clean_headr",
    "title": "doubleheadR",
    "section": "Step 3: clean_headr",
    "text": "Step 3: clean_headr\nWe can call clean_headr on our data.frame or tibble object, which takes three arguments:\n* dat: a data.frame object (in this case, inherited from LHS)\n* rep_val: the repeated value as a character string. Our column names have a repeated value of ‘…’ (or ‘..’, or ‘.’)\n* clean_names: this is a janitor function that will convert all column names to snake_case and will strip out any non-alphanumeric characters.\n\ndemo %>% \n  clean_headr(rep_val = '...', clean_names = TRUE) %>% \n  colnames()\n\n [1] \"respondent_id\"                                          \n [2] \"please_provide_your_contact_information_name\"           \n [3] \"please_provide_your_contact_information_company\"        \n [4] \"please_provide_your_contact_information_address\"        \n [5] \"please_provide_your_contact_information_address_2\"      \n [6] \"please_provide_your_contact_information_city_town\"      \n [7] \"please_provide_your_contact_information_state_province\" \n [8] \"please_provide_your_contact_information_zip_postal_code\"\n [9] \"please_provide_your_contact_information_country\"        \n[10] \"please_provide_your_contact_information_email_address\"  \n[11] \"please_provide_your_contact_information_phone_number\"   \n[12] \"i_wish_it_would_have_snowed_more_this_winter_response\"  \n\n\nWe can compare this with clean_names = FALSE, which creates column names that are very similar to the actual survey questions that we’ve asked.\n\ndemo %>% \n  clean_headr(rep_val = '...', clean_names = FALSE) %>% \n  colnames()\n\n [1] \"Respondent ID\"                                           \n [2] \"Please provide your contact information: Name\"           \n [3] \"Please provide your contact information: Company\"        \n [4] \"Please provide your contact information: Address\"        \n [5] \"Please provide your contact information: Address 2\"      \n [6] \"Please provide your contact information: City/Town\"      \n [7] \"Please provide your contact information: State/Province\" \n [8] \"Please provide your contact information: ZIP/Postal Code\"\n [9] \"Please provide your contact information: Country\"        \n[10] \"Please provide your contact information: Email Address\"  \n[11] \"Please provide your contact information: Phone Number\"   \n[12] \"I wish it would have snowed more this winter. Response\""
  },
  {
    "objectID": "posts/doubleheadR/index.html#step-4-trim_headr",
    "href": "posts/doubleheadR/index.html#step-4-trim_headr",
    "title": "doubleheadR",
    "section": "Step 4: trim_headr",
    "text": "Step 4: trim_headr\nWhether we use clean_names or not, we will most likely want to shorten some of the column names so that our code is more legible. The demo data here is pretty conservative - if you have a verbose survey question, you can imagine how long your column names could be!\n\ntrim_headr makes it easy to shorten column names, and is really just composed of a concatenated gsub call on all of the column names. Let’s say we do use clean_names and we want to remove “please_provide_your_contact_” from our column names, so that we will still be left with shorter, descriptive names like information_name, information_company, etc. Let’s also shorten our last column name to snowed_more_this_winter.\n\ndemo %>% \n  clean_headr(rep_val = '...') %>% \n  trim_headr(c('please_provide_your_contact_', 'i_wish_it_would_have_', '_response')) %>% \n  colnames()\n\n [1] \"respondent_id\"               \"information_name\"           \n [3] \"information_company\"         \"information_address\"        \n [5] \"information_address_2\"       \"information_city_town\"      \n [7] \"information_state_province\"  \"information_zip_postal_code\"\n [9] \"information_country\"         \"information_email_address\"  \n[11] \"information_phone_number\"    \"snowed_more_this_winter\"    \n\n\nEasy! Depending on the number of survey questions, getting to this point could take a really long time by first manually renaming columns and then deleting the first row. I’ve found this workflow to be more intuitive, and leaves me more time to focus on the data and creating effective dashboards and reports.\nIf you happen to try out doubleheadr and find it helpful, or more likely, find some bugs or inefficiencies, I would love to hear about it!"
  },
  {
    "objectID": "posts/sorta/index.html",
    "href": "posts/sorta/index.html",
    "title": "A Sort(a) Useful Trick",
    "section": "",
    "text": "For that reason, this post will be extremely brief, and will just show a useful trick that uses dput(), which was unfamiliar to me until I started looking for ways to contribute to the R tag on stackoverflow.\nHere’s the sort(a)-scenario:\nLet’s say you have a vector that’s built into your code that you’d like to sort. I’ve been stuck on an idea for a blog post about Saves the Day, and have a vector that contains body parts.\n\nparts <- c(\"neck\", \"collarbone\", \"ankle\", \"thigh\", \"eyelid\")\n\nI like order in the world, and want to build this vector alphabetically, which will also make it easier to read as it’s presented in a blog post.\nInstead of doing your A-B-C’s and manually rewriting the vector, just use dput()!\n\ndput(sort(parts))\n\nc(\"ankle\", \"collarbone\", \"eyelid\", \"neck\", \"thigh\")\n\n\nEasy!\nNow we can just copy/paste and move on with our lives!\n\nordered_parts <- c(\"ankle\", \"collarbone\", \"eyelid\", \"neck\", \"thigh\")"
  },
  {
    "objectID": "projects/nuforcbot.html",
    "href": "projects/nuforcbot.html",
    "title": "nuforcbot",
    "section": "",
    "text": "Nuforcbot is a Twitter bot that scrapes NUFORC’s publicly available data and tweets about it. Everything runs off of R and GitHub Actions to automate a tweet every hour. The GitHub repo can be found here."
  },
  {
    "objectID": "projects/phillymetaldata.html",
    "href": "projects/phillymetaldata.html",
    "title": "phillymetaldata R Package",
    "section": "",
    "text": "If your R sessions aren’t metal enough, look no further. 🤘\nThis package uses the supabase API to pull data directly from the source, so real-time data is always available."
  },
  {
    "objectID": "projects/phillymetal.html",
    "href": "projects/phillymetal.html",
    "title": "phillymetal.net",
    "section": "",
    "text": "As a fan of heavy music and wanting to learn JavaScript, this site is built with mostly vanilla JS, HTML, and CSS, with a backend"
  },
  {
    "objectID": "projects/surveymonkey.html",
    "href": "projects/surveymonkey.html",
    "title": "surveymonkey R Package",
    "section": "",
    "text": "I inherited the surveymonkey R package from Sam Firke (formerly at TNTP) and have maintained the package since 2021. The surveymonkey package lets R users query the SurveyMonkey API directly to bring survey responses into R for quick and easy analysis."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I’m a Data Solutions Engineer @ Atorus. My goal is to share trivial and occasionally useful R content, learn from and connect with others in the data community, and contribute to open source. I live in Philadelphia with my amazing wife Molly, our two cats, Mango and Phil, and our dog Donut. As a huge fan of heavy music, I also run phillymetal.net, a website that helps people in the Philadelphia area find metal shows 🤘."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nsnippet\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2022\n\n\nMatt Roumaya\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nreporting\n\n\nRMarkdown\n\n\nMicrosoft Access\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2021\n\n\nMatt Roumaya\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ncats\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2021\n\n\nMatt Roumaya\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nR package\n\n\nmetal\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2020\n\n\nMatt Roumaya\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nsurvey\n\n\nSurveyMonkey\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2020\n\n\nMatt Roumaya\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nsurvey\n\n\nSurveyMonkey\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2020\n\n\nMatt Roumaya\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ndataviz\n\n\ncats\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2020\n\n\nMatt Roumaya\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/phillymetal/index.html",
    "href": "posts/phillymetal/index.html",
    "title": "phillymetaldata R package",
    "section": "",
    "text": "With the immense help of my amazingly talented colleage and R/JavaScript developer Maya Gans, I’ve jumped head first into the world of JavaScript and am totally hooked. In future posts, I’m planning to write up my experience of learning JavaScript as a (primarily) R user.\nEven though I’m stoked about JS, I’m still just as much of an R nerd as ever, and wanted to make all of the data on phillymetal.net freely available whether it’s in R, a Google Sheet, .csv file, or any other way that could help people in the Philadelphia area find cool shows to go to. So for now, I’m excited to introduce a one-function R package that makes all of the data super easy to pull.\nIntroducing the {phillymetaldata} package. You can find the repo here!\n\n\n\n\n\n\nInstall {phillymetaldata}\nTo install {phillymetaldata}, run:\n\ndevtools::install_github('mattroumaya/phillymetaldata')\n\n\n\nGet Data\nGetting data is as easy as typing “get_data()”. In fact, that’s all you need to do:\n\nlibrary(phillymetaldata)\nsuppressMessages(library(tidyverse))\nlibrary(gt)\n\ndata <- phillymetaldata::get_data()\n\nIn the table below, you can see a consolidated view of the data available from get_data(). I removed the unique key/id, whether the entry was validated, and the URL, just to make it a little cleaner to view.\n\ndata %>% \n  head(n = 10) %>% \n  select(-c(id, validated, url)) %>% \n  gt::gt() %>% \n    cols_width(\n    added ~ px(150),\n    show_date ~ px(200),\n    description ~ px(250),\n    venue ~ px(250)\n  )\n\n\n\n\n\n  \n    \n    \n    \n    \n  \n  \n  \n    \n      added\n      show_date\n      description\n      venue\n    \n  \n  \n    2022-04-11\n2022-04-09\nJulia's War Fest Day 2\nUkie Club\n    2022-04-11\n2022-04-10\nJulia's War Fest Day 3\nUkie Club\n    2022-04-11\n2022-04-23\nIllusions Of Grandeur // Empress // Sequoia Grove\nDobbs on South\n    2022-04-11\n2022-04-30\nTraitor // Roadkiller // Brazen Hell // Skullovich\nKung Fu Necktie\n    2022-04-11\n2022-04-30\nTestament // Exodus // Death Angel\nStarland Ballroom\n    2022-04-11\n2022-04-30\nCavern Womb // Coagulate // Castrator // Viogression // Mortuous // Scattered Remnants\nWarehouse on Watts\n    2022-04-11\n2022-05-01\nBastard Cross // Night Hag // Soul Devourment // Blood Spore // Ectovoid // Phobophilic // Vastum\nWarehouse on Watts\n    2022-04-11\n2022-05-06\nKnocked Loose // Movements // Kublai Khan // Koyo\nStarland Ballroom\n    2022-04-11\n2022-05-06\nClamfight // Wax Brain // LMI // Endless Teeth // Fright\nCentury\n    2022-04-11\n2022-05-24\nNine Inch Nails\nThe Met\n  \n  \n  \n\n\n\n\n\n\nLooking For a Show?\nIf you happen to be an R user and metalhead/punk/heavy music lover, and also happen to be in the Philadelphia area, you can look for shows to check out by filtering for upcoming shows. (You can also just check out phillymetal.net).\nTo do this, just use the upcoming_shows_only parameter:\n\nget_data(upcoming_shows_only = TRUE) %>% \n  head(n = 10) %>% \n  select(show_date, description, venue) %>% \n  arrange(show_date) %>% \n  gt::gt() %>% \n      cols_width(\n    show_date ~ px(200),\n    description ~ px(400),\n    venue ~ px(250)\n  )\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n  \n    \n      show_date\n      description\n      venue\n    \n  \n  \n    2022-10-23\nChat Pile // Orphan Donor // Planning For Burial\nUnderground Arts\n    2022-10-25\nDead Boys // Suzi Moon // Tone Bandits\nKung Fu Necktie\n    2022-10-26\nAsagraum // Cultus Profano // IATT // Blasphemous // Sakrilejist\nThe Fire\n    2022-11-04\nAFI // Drab Majesty\nFranklin Music Hall\n    2022-11-05\nRussian Circles // REZN\nWorld Cafe Live\n    2022-11-06\nOff! // Zulu\nFirst Unitarian Church\n    2022-11-13\nBlack Flag // T.S.O.L. // The Dickies // Total Chaos\nStarland Ballroom\n    2022-11-20\nSaetia // Soul Glo // Massa Nera \nFirst Unitarian Church\n    2022-11-22\nAmon Amarth // Carcass // Obituary // Cattle Decapitation\nThe Fillmore\n    2022-12-05\nBlitzkid\nMilkBoy\n  \n  \n  \n\n\n\n\nThat’s all I have for now!\nI’m going to see Chat Pile tomorrow so you should check them out too:"
  }
]